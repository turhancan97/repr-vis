{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692cffd-3fbc-4d92-be2b-c13572851e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import lovely_tensors\n",
    "lovely_tensors.monkey_patch()\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from utils.misc import convert_mae_to_vit, convert_maskfeat_to_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "repr_dic = \"/shared/results/common/kargin/unreal_engine/features/initial_test\"\n",
    "model_dic = \"/shared/results/common/kargin/unreal_engine/models\"\n",
    "dataset_dic = \"/shared/results/common/kargin/unreal_engine/dataset/initial_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddaaa4b7-99ba-4ae0-b5e7-b94dd926bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dummy\"\n",
    "\n",
    "class DummyModel:\n",
    "    def forward_features(self, x):\n",
    "        return torch.rand(x.size(0), 197, 768)\n",
    "    def eval(self):\n",
    "        pass\n",
    "\n",
    "model = DummyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fdb09b-6e0a-49d7-aebf-fb41461a0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"croco\"\n",
    "\n",
    "state_dict = torch.load(f'{model_dic}/CroCo.pth', 'cpu')\n",
    "state_dict = state_dict[\"model\"]\n",
    "state_dict = convert_mae_to_vit(state_dict)\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=False).cuda()\n",
    "msg = model.load_state_dict(state_dict, strict=False)\n",
    "use_gpu = torch.cuda.is_available() and torch.cuda.device_count()>0\n",
    "device = torch.device('cuda:0' if use_gpu else 'cpu')\n",
    "model = model.eval()\n",
    "model = model.to(device=device)\n",
    "model.head = torch.nn.Identity()\n",
    "model.fc_norm = torch.nn.Identity()\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64008839",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mae\"\n",
    "\n",
    "state_dict = torch.load(f'{model_dic}/mae_pretrain_vit_base.pth', 'cpu')\n",
    "state_dict = state_dict[\"model\"]\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=False).cuda()# msg = model.load_state_dict(state_dict, strict=False)\n",
    "msg = model.load_state_dict(state_dict, strict=False)\n",
    "use_gpu = torch.cuda.is_available() and torch.cuda.device_count()>0\n",
    "device = torch.device('cuda:0' if use_gpu else 'cpu')\n",
    "model = model.eval()\n",
    "model = model.to(device=device)\n",
    "model.head = torch.nn.Identity()\n",
    "model.fc_norm = torch.nn.Identity()\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e84f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"maskfeat\"\n",
    "\n",
    "state_dict = torch.load(f'{model_dic}/in1k_VIT_B_MaskFeat_PT_epoch_01600.pyth', 'cpu')\n",
    "state_dict = state_dict['model_state']\n",
    "state_dict = convert_maskfeat_to_vit(state_dict)\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=False).cuda()\n",
    "msg = model.load_state_dict(state_dict, strict=False)\n",
    "use_gpu = torch.cuda.is_available() and torch.cuda.device_count()>0\n",
    "device = torch.device('cuda:0' if use_gpu else 'cpu')\n",
    "model = model.eval()\n",
    "model = model.to(device=device)\n",
    "model.head = torch.nn.Identity()\n",
    "model.fc_norm = torch.nn.Identity()\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962bb04-b526-44de-be61-e7953ad5bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"spa\"\n",
    "\n",
    "state_dict = torch.load(f\"{model_dic}/spa-b.ckpt\")\n",
    "state_dict = state_dict[\"state_dict\"]\n",
    "\n",
    "for k in list(state_dict.keys()):\n",
    "    if not k.startswith(\"model.img_backbone\"):\n",
    "        del state_dict[k]\n",
    "    else:\n",
    "        new_key = k.replace(\"model.img_backbone.\", \"\")\n",
    "        state_dict[new_key] = state_dict.pop(k)\n",
    "\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=False).cuda()\n",
    "model.head = torch.nn.Identity()\n",
    "model.fc_norm = torch.nn.Identity()\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da0cb1ce-01ac-435c-a14a-7847665544cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"dino\"\n",
    "\n",
    "model = timm.create_model(f'vit_base_patch16_224.{model_name}', pretrained=True).cuda()\n",
    "model.head = torch.nn.Identity()\n",
    "model.fc_norm = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5de8573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"deit\"\n",
    "model = timm.create_model('deit3_base_patch16_224.fb_in1k', pretrained=True).cuda()\n",
    "model.head = torch.nn.Identity()\n",
    "model.fc_norm = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "feeff48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"clip\"\n",
    "model = timm.create_model('vit_base_patch16_clip_224.laion2b', pretrained=True).cuda()\n",
    "model.head = torch.nn.Identity()\n",
    "model.fc_norm = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93912d8e-25b8-4345-ad84-d8e619c332ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.samples = list(self.root.rglob(\"*.jpg\"))  \n",
    "        # self.samples = [p for p in self.samples if \".ipynb_checkpoints\" not in str(p)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.samples[index]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        try:\n",
    "            object_class = path.parent.name\n",
    "            environment = path.parent.parent.name\n",
    "            orientation = path.parent.parent.parent.name.replace(\"Frames_\", \"\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Unexpected folder structure for {path}\") from e\n",
    "\n",
    "        match = re.search(r\"(\\d+)\\.jpg$\", path.name)\n",
    "        file_number = int(match.group(1)) if match else -1\n",
    "\n",
    "        metadata = {\n",
    "            \"path\": str(path),\n",
    "            \"orientation\": orientation,\n",
    "            \"environment\": environment,\n",
    "            \"object_class\": object_class,\n",
    "            \"number\": file_number\n",
    "        }\n",
    "\n",
    "        return image, metadata\n",
    "\n",
    "dataset = HierarchicalDataset(\n",
    "    f\"{dataset_dic}/\",\n",
    "    transform=T.Compose([\n",
    "        T.Resize(224),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "    ])\n",
    ")\n",
    "    \n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b52f2f4-dba2-4bc5-a20c-554045c789cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model_name)\n",
    "result = {\n",
    "    \"features\": [],\n",
    "    \"orientation\": [],\n",
    "    \"environment\": [],\n",
    "    \"object_class\": [],\n",
    "    \"number\": [],\n",
    "    \"path\": [],\n",
    "}\n",
    "\n",
    "model.eval()\n",
    "for images, metadata in tqdm(dataloader):\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = model.forward_features(images)\n",
    "\n",
    "    result[\"features\"].extend(feats.cpu())\n",
    "    for key in metadata:\n",
    "        result[key].extend(metadata[key])\n",
    "\n",
    "result[\"features\"] = torch.stack(result[\"features\"]).half()\n",
    "\n",
    "torch.save(result, f\"{repr_dic}/repr_{model_name}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lynx-reid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
